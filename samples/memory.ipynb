{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from pprint import pprint\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryMemory, ConversationEntityMemory\n",
    "from langchain.memory import CombinedMemory\n",
    "\n",
    "# read local .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "\n",
    "\n",
    "# setup azure openai api\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key  = os.getenv('AZURE_OAI_KEY')\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "deployment_name_gpt = os.getenv(\"AZURE_OAI_MODEL_GPT_3\")\n",
    "deployment_name_ada = os.getenv(\"AZURE_OAI_MODEL_ADA\")\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = openai.api_type\n",
    "os.environ[\"OPENAI_API_BASE\"] = openai.api_base\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "os.environ[\"OPENAI_API_VERSION\"] = openai.api_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "llm = AzureOpenAI(deployment_name=deployment_name_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Memory\n",
    "conv_memory = ConversationBufferWindowMemory(\n",
    "    k=3,\n",
    "    input_key=\"input\",\n",
    ")\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key='summary',\n",
    "    input_key=\"input\",\n",
    ")\n",
    "\n",
    "# Combined\n",
    "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
    "TEMPLATE = \"\"\"\n",
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Summary of conversation so far:\n",
    "{summary}\n",
    "\n",
    "Last 3 lines of conversation:\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"history\", \"input\"], template=TEMPLATE\n",
    ")\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=memory,\n",
    "    prompt=PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(conversation.run(input=\"Hello! How are you doing?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
